{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "d7g50m4hyawyv12fhrprpj"
   },
   "outputs": [],
   "source": [
    "%pip install pymorphy2\n",
    "%pip install stt_metrics-0.12-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "78omt64naxwj6e98h66w5n"
   },
   "source": [
    "# Обучение языковых моделей для STT\n",
    "\n",
    "## Пошаговый гайд\n",
    "\n",
    "1. Подготовьте данные, необходимые для обучения языковой модели. Для этого требуется подготовить тексты, а также наборы из размеченных валидационных и тестовых аудиозаписей\n",
    " \n",
    "  * Набор текстов, необходимый для обучения языковой модели, должен представлять из себя таблицу в формате TSV, состоящую из двух столбцов, где второй столбец соответствует самим текстам (предложениям), а первый столбец &mdash; количеству вхождений этих текстов в исходный датасет. Также к текстам разметки предъявляются следующие требования:\n",
    " \n",
    "    * все тексты должны быть приведены к нижнему регистру\n",
    "    \n",
    "    * тексты не должны содержать символов, отличных от символов русского алфавита и пробелов\n",
    "    \n",
    "    * тексты не должны содержать букв \"ё\", все такие буквы следует заменять на \"е\"\n",
    "    \n",
    "    * тексты не должны содержать пробелов в начале и в конце, также все слова должны быть разделены одним пробелом\n",
    "    \n",
    "    Следует отметить, что аналогичные требования также предъявляются и к разметке аудиозаписей\n",
    " \n",
    "  * Валидационные аудиозаписи и их разметка должны быть сохранены в отдельной папке. Эти данные будут использоваться для подбора гиперпараметров обучаемой модели.\n",
    " \n",
    "    Все аудиозаписи должны иметь следующий формат:\n",
    "    \n",
    "    * одноканальное аудио в формате WAV\n",
    "    \n",
    "    * sample rate: 8000, 16000 или 48000\n",
    "    \n",
    "    * разрядность: 16 бит, little endian\n",
    " \n",
    "    Кроме того, вместе с валидационными аудиозаписями должен храниться файл `records.json` с разметкой аудио. Этот файл должен иметь следующий формат:\n",
    "    \n",
    "    ```\n",
    "    {\n",
    "        \"<audio1_name>.wav\": \"разметка 1-ой записи\",\n",
    "        ...\n",
    "        \"<audioN_name>.wav\": \"разметка N-ой записи\",\n",
    "    }\n",
    "    ```\n",
    "    &nbsp;\n",
    "  * Тестовые аудиозаписи и их разметка должны быть подготовлены аналогично набору валидационных записей. Эти данные не будут использоваться при обучении, но по ним будет получены распознавания с помощью обученной языковой модели, которые затем могут быть использованы для оценки её качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "812jsg83gv958gmu2zg2j5"
   },
   "source": [
    "2. Запустите команду\n",
    "   \n",
    "  ```\n",
    "    > #!nirvana\n",
    "    > sk_train_language_model --train-texts PATH --validation-dir PATH --test-dir PATH --model PATH --recognitions PATH\n",
    "    >\n",
    "  ```\n",
    "  * `--train-texts PATH` &mdash; путь до TSV-таблицы с текстами\n",
    "  \n",
    "  * `--validation-dir PATH` &mdash; путь до директории с валидационными аудиозаписями\n",
    "  \n",
    "  * `--test-dir PATH` &mdash; путь до директории с тестовыми аудиозаписями\n",
    "  \n",
    "  * `--model PATH` &mdash; путь, по которому будет сохранена полученная языковая модель\n",
    "  \n",
    "  * `--recognitions PATH` &mdash; путь, по которому будет сохранены распознавания аудиозаписей из тестового набора (в формате JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "i3fnc2187amw38rmpebim9"
   },
   "source": [
    "3. Дождитесь завершения команды. На этом этапе вы также можете закрыть вкладку с ноутбуком и вернуться за результатам позже. Обучение модели отработает в фоновом режиме.\n",
    "\n",
    "   Время обучения напрямую зависит от количества предоставленных текстов, а также от количества аудиозаписей. \n",
    "   \n",
    "   Количество данных, требуемое для обучения хорошей языковой модели, может сильно варьироваться в зависимости от конкретной задачи. Тем не менее, _рекомендуется_ использовать не менее 1МБ текстов, а также от 100 до 2000 валидационных и тестовых аудиозаписей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xcdl2yawqsyt76h7v18rq"
   },
   "source": [
    "## Подготовка данных\n",
    "\n",
    "Приведём пример того, как подготовить данные для обучения кастомной языковой модели.\n",
    "\n",
    "Предположим, что изначально в нашем распоряжении имеется набор текстов `raw_data/dataset.txt`, набор аудиозаписей, хранящийся в папке `raw_data/audio`, а также разметка этих аудиозаписей, хранящаяся в папке `raw_data/references`.\n",
    "\n",
    "### Подготовка текстов\n",
    "\n",
    "В первую очередь подготовим тексты для обучения языковой модели. Для этого разобьём тексты нашего датасета на предложения, нормализуем их и сохраним в файл `prepared_data/texts.tsv` в соответствии с описанным выше форматом. \n",
    "\n",
    "Ниже приведён возможный пример такой обработки текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "roax91mx559j2vf76byqvq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def normalize_sentence(text: str) -> str:\n",
    "    # приводим текст предложения к нижнему регистру, заменяем буквы \"ё\" на \"е\"\n",
    "    text = text.lower().replace('ё', 'е')\n",
    "    \n",
    "    # удаляем из текста спец. символы, оставляем только русскоязычные символы и пробелы\n",
    "    text = re.sub(r'[^а-я ]', ' ', text)\n",
    "    \n",
    "    # также удаляем все лишние пробелы\n",
    "    text = ' '.join(filter(len, text.split()))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def prepare_texts(src_path: Path, dst_path: Path, merge_duplicates: bool = True):\n",
    "    sentences = []\n",
    "    \n",
    "    text = src_path.read_text()\n",
    "    \n",
    "    # разбиваем текст на предложения и нормализуем их\n",
    "    for paragraph in text.splitlines():\n",
    "        for sentence in sent_tokenize(paragraph):\n",
    "            sentences.append(normalize_sentence(sentence))\n",
    "\n",
    "    if not dst_path.parent.exists():\n",
    "        dst_path.parent.mkdir(parents=True)\n",
    "        \n",
    "    with dst_path.open('w') as f:\n",
    "        if not merge_duplicates:\n",
    "            # в простом варианте сохраняем все предложения отдельно, допуская дубликаты\n",
    "            for sentence in sentences:\n",
    "                f.write(f'1\\t{sentence}\\n')\n",
    "        else:\n",
    "            # опционально можем избавиться от дубликатов, сохранив с каждым предложением количество его вхождений в текст\n",
    "            sentence_count = defaultdict(int)\n",
    "            for sentence in sentences:\n",
    "                sentence_count[sentence] += 1\n",
    "            for sentence, count in sentence_count.items():\n",
    "                f.write(f'{count}\\t{sentence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "5msf6puc3cenctsa2kg58f"
   },
   "outputs": [],
   "source": [
    "prepare_texts(src_path=Path('raw_data', 'dataset.txt'), dst_path=Path('prepared_data', 'texts.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "anulmrh9nir3v4jha06xmm"
   },
   "source": [
    "### Подготовка аудио\n",
    "\n",
    "Далее подготовим аудиозаписи. \n",
    "\n",
    "Для этого разобъём весь набор аудиозаписей на валидационный и тестовый наборы, а затем переведём аудиозаписи и их разметку в требуемый нам формат, разместив их в папках `prepared_data/val_audio` и `prepared_data/test_audio` соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "cnxmza5cmrnrlqxv84cjd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.24.1-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.24.1\n",
      "\u001b[33mWARNING: Target directory /home/jupyter/work/pyenv/pydub already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/jupyter/work/pyenv/pydub-0.24.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pydub\n",
    "\n",
    "import random\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def prepare_audio_set(src_audio_dir: Path, reference_paths: List[Path], dst_dir: Path):\n",
    "    if not dst_dir.exists():\n",
    "        dst_dir.mkdir(parents=True)\n",
    "    \n",
    "    records_desc = {}\n",
    "    \n",
    "    for reference_path in reference_paths:\n",
    "        # по имени файла с разметкой получаем путь до соответствующей аудиозаписи\n",
    "        audio_name = reference_path.name.replace('.txt', '.wav')\n",
    "        \n",
    "        src_audio_path = src_audio_dir / audio_name\n",
    "        dst_audio_path = dst_dir / audio_name\n",
    "        \n",
    "        with src_audio_path.open('rb') as f:\n",
    "            audio = AudioSegment.from_wav(f)\n",
    "            \n",
    "        # приводим все аудиозаписи к единому формату, с которым работают наши инструменты\n",
    "        audio = audio.set_channels(1).set_sample_width(2).set_frame_rate(16000)\n",
    "        \n",
    "        # и сохраняем в формате WAV\n",
    "        with dst_audio_path.open('wb') as f:\n",
    "            audio.export(out_f=f, format='wav')\n",
    "        \n",
    "        # кроме того, нормализуем текст разметки\n",
    "        records_desc[audio_name] = normalize_sentence(reference_path.read_text())\n",
    "    \n",
    "    with (dst_dir / 'records.json').open('w') as f:\n",
    "        json.dump(records_desc, f, indent=2)\n",
    "\n",
    "\n",
    "# здесь получим список всех записей (их разметок) и поровну разделяем его на валидационный и тестовый наборы\n",
    "def prepare_audio(src_audio_dir: Path, src_reference_dir: Path, dst_dir: Path):\n",
    "    random.seed(0)\n",
    "    \n",
    "    reference_paths = list(filter(lambda f: f.name.endswith('.txt'), src_reference_dir.iterdir()))\n",
    "    random.shuffle(reference_paths)\n",
    "    \n",
    "    split_size = len(reference_paths) // 2\n",
    "    \n",
    "    prepare_audio_set(src_audio_dir, reference_paths[:split_size], dst_dir / 'val_audio')\n",
    "    prepare_audio_set(src_audio_dir, reference_paths[split_size:], dst_dir / 'test_audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "9a0raeht69f9u9i4rfmhke"
   },
   "outputs": [],
   "source": [
    "prepare_audio(Path('raw_data', 'audio'), Path('raw_data', 'references'), Path('prepared_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "57xupfyjmfhrrlo7n7v8y7"
   },
   "source": [
    "## Обучение модели\n",
    "\n",
    "Теперь мы можем вызвать команду `sk_train_language_model`, указав в качестве входных параметров пути до текстов и наборов аудиозаписей.\n",
    "\n",
    "По завершению исполнения ячейки в файл `lm` будет сохранена языковая модель, а в файл `recognitions.json` будет сохранены результаты потокового распознавания записей из тестового набора в зависимости от некоторых настроек распознавания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "906235urfja1w7jkni7ts"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training texts from ./prepared_data/texts.tsv\n",
      "Preparing validation audio files from ./prepared_data/val_audio\n",
      "Preparing test audio files from ./prepared_data/test_audio\n",
      "Running training process...\n",
      "Training process has finished.\n",
      "Language model is saved to lm\n",
      "Results calculated by this model and test data set are saved to recognitions.json\n"
     ]
    }
   ],
   "source": [
    "#!nirvana\n",
    "sk_train_language_model --train-texts ./prepared_data/texts.tsv --validation-dir ./prepared_data/val_audio --test-dir ./prepared_data/test_audio --model lm --recognitions recognitions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "lob1u0qv2yovas0d5lyu3"
   },
   "source": [
    "## Оценка качества модели\n",
    "\n",
    "Наконец, мы можем оценить качество полученной модели на тестовой выборке, чтобы затем иметь возможность выбрать лучшую модель.\n",
    "\n",
    "Подробнее о том, как \"правильно\" оценить качество модели, рассказано [тут](?). Здесь же мы приведём самый простой вариант оценки качества модели в зависимости от использованных параметров распознавания с помощью метрики WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "txlpmi7cil4segayo9w2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition params: {'noise_reduction': 'Heavy'}\n",
      "Mean WER: 0.2857142857142857\n",
      "Recognition params: {'noise_reduction': 'Normal'}\n",
      "Mean WER: 0.09903381642512077\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from stt_metrics import evaluate_wer\n",
    "\n",
    "references = json.loads(Path('prepared_data', 'test_audio', 'records.json').read_text())\n",
    "recognition_sets = json.loads(Path('recognitions.json').read_text())\n",
    "\n",
    "for recognition_set in recognition_sets:\n",
    "    mean_wer, full_report = evaluate_wer(references=references, hypotheses=recognition_set['recognitions'])\n",
    "    print(f'Recognition params: {recognition_set[\"params\"]}')\n",
    "    print(f'Mean WER: {mean_wer}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kbb944hdlpkij3cj3syc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "fa305887-95eb-4fa8-805a-370bbff8d3e8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
